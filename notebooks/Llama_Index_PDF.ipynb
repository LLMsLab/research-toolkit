{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the environment variables from the local .env file\n",
    "# NOTE: while using Jupyter Notebooks with VS Code, even when the .env\n",
    "# is located in the root directory of the project, you must use ../.env\n",
    "# instead of .env\n",
    "load_dotenv(\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: not using Google Colab\n"
     ]
    }
   ],
   "source": [
    "# Check if the .env file exists in the Google Drive path\n",
    "if os.path.exists(\"/content/drive/MyDrive/Projects/.env\"):\n",
    "    load_dotenv(\"/content/drive/MyDrive/Projects/.env\")\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google Colab\")\n",
    "else:\n",
    "    COLAB = False\n",
    "    print(\"Note: not using Google Colab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving API keys from the environment\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='UNITED STATES\\nSECURITIES AND EXCHANGE COMMISSION\\nWashington, D.C. 20549\\nFORM 10-Q\\n(Mark One)\\n☒    QUARTERLY REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934\\nFor the quarterly period ended December\\xa025, 2021\\nor\\n☐    TRANSITION REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934\\nFor the transition period from \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0  to \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 .\\nCommission File Number: 001-36743\\nApple Inc.\\n(Exact name of Registrant as specified in its charter)\\nCalifornia 94-2404110\\n(State or other jurisdiction\\nof incorporation or organization)(I.R.S. Employer Identification No.)\\nOne Apple Park Way\\nCupertino , California 95014\\n(Address of principal executive offices) (Zip Code)\\n(408) 996-1010\\n(Registrant’s telephone number, including area code)\\nSecurities registered pursuant to Section 12(b) of the Act:\\nTitle of each classTrading \\nsymbol(s) Name of each exchange on which registered\\nCommon Stock, $0.00001  par value per share AAPL The Nasdaq Stock Market LLC', metadata={'source': '../data/raw/Apple-Financial-Report-Q1-2022.pdf', 'page': 0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## load the PDF using pypdf\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# load the data\n",
    "loader = PyPDFLoader(\n",
    "    \"../data/raw/Apple-Financial-Report-Q1-2022.pdf\"\n",
    ")\n",
    "\n",
    "# the 10k financial report are huge, we will need to split the doc into multiple chunk.\n",
    "# This text splitter is the recommended one for generic text. It is parameterized by a list of characters.\n",
    "# It tries to split on them in order until the chunks are small enough.\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=0\n",
    ")\n",
    "data = loader.load()\n",
    "texts = text_splitter.split_documents(data)\n",
    "\n",
    "# view the first chunk\n",
    "texts[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Question Answering\n",
    "\n",
    "Now I know we are going to use OpenAI as LLM Provider so it makes total sense that we should go with OpenAI Embedding. But please  **note**  that the OpenAI Embedding API use  **“text-davinci-003”**  model, you can view the pricing  [here](https://openai.com/pricing), it may cost less for a small document but be careful when you intend to apply for a big chunk of documents (don’t break your bank guys).\n",
    "\n",
    "**NExt steps**, we will import the  [Chroma](https://docs.trychroma.com/). If you are not familiar with Chroma, then you can find the detail on its official website. Again, I will cover Chroma and its alternative sometime in the future. So the question is, what is Chroma and why do we need it?\n",
    "\n",
    "In short, Chroma is the embedding database, not like the traditional SQL database or the not-too-new NoSQL database like what you usually work with. It is embedding databases and it makes it easy to build LLM apps.\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:699/0*-4HPqxvt3UmR-iSN.png)\n",
    "\n",
    "By Chroma Official Website\n",
    "\n",
    "Our document is represented in the form of text which makes it challenging to find relevant info based on the question. Say you need to find the revenue of Apple in the last quarter in 1000 pages and compare revenue to previous years. How challenging and time-consuming it may take? So to make our search easier, we will first need to transform or represent words or phrases in a numerical format that can be used as input to machine learning models. In other words, to help machines understand the text. An embedding maps each word or phrase to a vector of real numbers, typically with hundreds of dimensions, such that similar words or phrases are mapped to similar vectors in the embedding space.\n",
    "\n",
    "One of the main advantages of using embeddings is that they can capture the semantic and syntactic relationships between words or phrases. For example, in an embedding space, the vectors for “king” and “queen” would be closer to each other than to the vector for “apple”, because they are semantically related as royal titles.\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:700/0*mijTnoEZJI7qqfBl.png)\n",
    "\n",
    "So, the embedding database does exactly that. It will store all the embedding data in the database and then give us very indexes to allow us to perform an action like data retrieval and do it in a scalable style. If you need to get the answer to the previous question of finding revenue of Apple last quarter, we will first need to perform a similarity search or semantic search on top of embedding a database like Chroma to extract relevant information and feed that information to LLM model to get the answer.\n",
    "\n",
    "Sounds too complex !! that is where Langchain comes to the rescue with all the hard work will be done in the background for us. Let’s start coding, shall we?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Chroma and OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "# initialize OpenAIEmbedding\n",
    "embeddings = OpenAIEmbeddings(model='text-embedding-ada-002')\n",
    "\n",
    "# use Chroma to create in-memory embedding database from the doc\n",
    "docsearch = Chroma.from_documents(texts, embeddings,  metadatas=[{\"source\": str(i)} for i in range(len(texts))])\n",
    "\n",
    "## perform search based on the question\n",
    "query = \"What is the operating income?\"\n",
    "docs = docsearch.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Operating income $ 3,349 $ 3,503 \\nRest of Asia Pacific:\\nNet sales $ 9,810 $ 8,225 \\nOperating income $ 3,995 $ 2,953 \\nA reconciliation of the Company’s segment operating income to the Condensed Consolidated Statements of Operations for the \\nthree months ended December 25, 2021 and December 26, 2020  is as follows (in millions):\\nThree Months Ended\\nDecember 25,\\n2021December 26,\\n2020\\nSegment operating income $ 49,657 $ 40,360 \\nResearch and development expense  (6,306)  (5,163) \\nOther corporate expenses, net  (1,863)  (1,663) \\nTotal operating income $ 41,488 $ 33,534 \\nApple Inc. | Q1 2022  Form 10-Q | 13', metadata={'source': '../data/raw/Apple-Financial-Report-Q1-2022.pdf', 'page': 15})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaining the code\n",
    "\n",
    "Sure, I'll explain what the given Python code does.\n",
    "\n",
    "1. Import Required Modules\n",
    "   ```python\n",
    "   from langchain.vectorstores import Chroma\n",
    "   from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "   ```\n",
    "   The first two lines import the necessary modules for the task. The `Chroma` class from the `langchain.vectorstores` module is used to create an in-memory database of text embeddings. The `OpenAIEmbeddings` class from the `langchain.embeddings.openai` module is used to generate these embeddings.\n",
    "\n",
    "2. Initialize OpenAI Embedding\n",
    "   ```python\n",
    "   embeddings = OpenAIEmbeddings(model='text-embedding-ada-002')\n",
    "   ```\n",
    "   Here, an instance of `OpenAIEmbeddings` is created using the model `'text-embedding-ada-002'`. This object, `embeddings`, will be used to convert text into numerical vectors that capture the semantic meaning of the words and phrases in the text.\n",
    "\n",
    "3. Create Chroma Object\n",
    "   ```python\n",
    "   docsearch = Chroma.from_documents(texts, embeddings, metadatas=[{\"source\": str(i)} for i in range(len(texts))])\n",
    "   ```\n",
    "   This line creates an instance of `Chroma` from a set of documents. The `from_documents` method takes the list of documents (`texts`), the embedding model (`embeddings`), and a list of metadata dictionaries. Each document is associated with a metadata dictionary, which in this case simply assigns an index to each document using the key `\"source\"`.\n",
    "\n",
    "   The `Chroma` instance `docsearch` serves as an in-memory database of the document embeddings. The embeddings are vectors in a high-dimensional space, where distances between vectors reflect the semantic similarity of the corresponding pieces of text.\n",
    "\n",
    "4. Perform Similarity Search\n",
    "   ```python\n",
    "   query = \"What is the operating income?\"\n",
    "   docs = docsearch.similarity_search(query)\n",
    "   ```\n",
    "   The `similarity_search` method of the `docsearch` object is used to find the documents that are most similar to a given query. In this case, the query is `\"What is the operating income?\"`. The method returns a list of documents (or their associated metadata) ordered by their similarity to the query.\n",
    "\n",
    "The code uses the Langchain library, which provides tools for using language models and embeddings to build natural language processing applications. In this case, the application is a question-answering system that uses OpenAI's language model and embeddings, and the Chroma embedding database, to find relevant information in a large collection of documents.\n",
    "\n",
    "The context you provided explains why an embedding database like Chroma is useful for this task. Given a question, it is challenging and time-consuming to find the relevant information in a large collection of text documents. The solution is to convert the text into numerical embeddings, which capture the semantic meaning of the words and phrases, and store these in a database. We can then perform a similarity search in the embedding space to quickly identify the documents that are most likely to contain the answer to the question. Then, this information can be passed to a language model to generate a response."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see we are able to perform a similarity search to get relevant information from the embedding database.\n",
    "\n",
    "Now, we will use one of the main components of Langchain which is Chain\n",
    "to incorporate LLM provider into our code. Again, I know it is hard to\n",
    "digest all of the concepts at once but hey, I will cover all of them in\n",
    "another post. Remember, the purpose of this guide is to build the\n",
    "question-answering bot. So just follow the step and if you are curious\n",
    "and can’t wait to dig more into details, feel free to go to Langchain’s\n",
    "official website. Valhalla awaits!!!!\n",
    "\n",
    "There are four types of pre-built question-answering chains:\n",
    "\n",
    "-   Question Answering:  **load_qa_chain**\n",
    "-   Question Answering with Sources:  **load_qa_with_sources_chain**\n",
    "-   Retrieval Question Answer:  **RetrievalQA**\n",
    "-   Retrieval Question Answering with Sources:  **RetrievalQAWithSourcesChain**\n",
    "\n",
    "They are pretty much similar, under the hood,  **RetrievalQA and RetrievalQAWithSourcesChain** use  **load_qa_chain and load_qa_with_sources_chain**  respectively, the only difference is the first two will take all the embedding to feed into LLM while the last two only feed LLM with relevant information. We can use the first two to extract the relevant information first and feed that info to LLM only. Also, the first two give us more flexibility than the last two.\n",
    "\n",
    "The following piece of code will demonstrate how we do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing necessary framework\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will try 4 different question-answering chains\n",
    "\n",
    "1. load_qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The operating income for the three months ended December 25, 2021, was $41,488 million.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## use LLM to get answering\n",
    "chain = load_qa_chain(\n",
    "    ChatOpenAI(temperature=0.2, model_name=\"gpt-3.5-turbo\"), chain_type=\"stuff\"\n",
    ")\n",
    "query = \"What is the operating income?\"\n",
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. load_qa_with_sources_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_text': 'The operating income for the three months ended December 25, 2021, was $41,488 million.\\nSOURCES: ../data/raw/Apple-Financial-Report-Q1-2022.pdf'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = load_qa_with_sources_chain(\n",
    "    ChatOpenAI(temperature=0.2, model_name=\"gpt-3.5-turbo\"), chain_type=\"stuff\"\n",
    ")\n",
    "query = \"What is the operating income?\"\n",
    "chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The operating income for the three months ended December 25, 2021, was $41,488 million.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(temperature=0.2, model_name=\"gpt-3.5-turbo\"),\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=docsearch.as_retriever(),\n",
    ")\n",
    "query = \"What is the operating income?\"\n",
    "qa.run(query)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. RetrievalQAWithSourcesChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'The operating income for the specified period is $41,488 million.\\n',\n",
       " 'sources': '../data/raw/Apple-Financial-Report-Q1-2022.pdf'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    ChatOpenAI(temperature=0.2, model_name=\"gpt-3.5-turbo\"),\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=docsearch.as_retriever(),\n",
    ")\n",
    "chain({\"question\": \"What is the operating income?\"}, return_only_outputs=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty easy ayy. Most of the code above is pretty basic. We just want to get this work done before digging into more depth about what does framework can offer. Until then, let’s move on to another framework that you can use in conjunction with Langchain and it will give you more power to create even better LLM apps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Apple supports the Aluminium Stewardship Initiative (ASI) and is a member of the organization. ASI focuses on promoting responsible sourcing within the aluminum value chain. Apple has recently completed an audit against ASI's Performance Standard, which includes environmental, social, and governance criteria. This engagement with ASI demonstrates Apple's commitment to advancing responsible practices in the aluminum industry.\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## use LLM to get answering\n",
    "chain = load_qa_chain(\n",
    "    ChatOpenAI(temperature=0.2, model_name=\"gpt-3.5-turbo\"), chain_type=\"stuff\"\n",
    ")\n",
    "query = \"What is the engagement with Aluminium Stewardship Initiative?\"\n",
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our colocation facilities\n",
      "The majority of our online services are provided by our own \n",
      "data centers; however, we also use third-party colocation \n",
      "facilities for additional data center capacity. While we don’t \n",
      "own these shared facilities and use only a portion of their total \n",
      "capacity, we include our portion of their energy use in our \n",
      "renewable energy goals.\n",
      "Starting in January 2018, 100 percent of our power for \n",
      "colocation facilities was matched with renewable energy \n",
      "generated within the same state or NERC region for facilities  \n",
      "in the United States, or within the same country or regional  \n",
      "grid for those around the world. As our loads grow over time, \n",
      "we’ll continue working with our colocation suppliers to match \n",
      "100 percent of our energy use with renewables.\n",
      "Furthermore, we worked with one of our main suppliers of \n",
      "colocation services to help it develop the capability to provide \n",
      "renewable energy solutions to its customers. This partnership \n",
      "advances Apple’s renewable energy program and those of \n",
      "other companies that use this colocation provider.\n",
      "Total energy  \n",
      "use (kWh)Renewable  \n",
      "energy (kWh)Default utility  \n",
      "emissions14 \n",
      " (metric tons CO 2e)Apple’s emissions –  \n",
      " including renewable  \n",
      "energy15 (metric tons CO 2e)Percent \n",
      "renewable energy16\n",
      "FY2011 42,500 0 10 10 0%\n",
      "FY2012 38,552,300 1,471,680 17,200 16,500 4%\n",
      "FY2013 79,462,900 46,966,900 31,800 14,500 59%\n",
      "FY2014 108,659,700 88,553,400 44,300 11,000 81%\n",
      "FY2015 142,615,000 121,086,100 60,500 12,700 85%\n",
      "FY201617145,520,900 143,083,200 66,300 1,600 98%\n",
      "FY2017 289,195,800 286,378,100 125,600 1,500 99%\n",
      "FY2018 327,663,800 326,959,700 146,600 400 99.8%\n",
      "FY2019 339,047,649 339,047,649 146,400 0 100%\n",
      "FY2020 372,901,398 372,901,398 153,459 0 100%\n",
      "FY2021 384,727,076 384,727,076 146,780 0 100%Third-party computing\n",
      "Beyond the use of our own data centers and colocation \n",
      "facilities, we also use third-party services to support some of \n",
      "our on-demand cloud computing and storage services. We \n",
      "are requiring these suppliers to adopt a 100 percent renewable \n",
      "energy strategy for their Apple energy use, and we continue \n",
      "to work with our suppliers to refine estimates for the carbon \n",
      "emissions associated with their services.\n",
      "14.  We calculate “default utility emissions” to provide baseline emissions of what \n",
      "our carbon footprint would have been without the use of renewable energy. \n",
      "This allows us to demonstrate the savings resulting from our renewable \n",
      "energy program.\n",
      "15.  Apple’s greenhouse gas emissions are calculated using the World Resources \n",
      "Institute Greenhouse Gas Protocol methodology for calculating market-\n",
      "based emissions.\n",
      "16.  We calculate our progress toward our 100 percent renewable energy goal on \n",
      "a calendar year basis, while the numbers reported in this table are based on \n",
      "fiscal year. Beginning January 1, 2018, all of the electricity use at our colocation \n",
      "facilities is from 100 percent renewable energy.17.  Over the past few years, we have been installing submeters in colocation \n",
      "facilities to better track electricity usage. Beginning in FY2016, we started \n",
      "reporting this submetered electricity usage. Prior to fiscal year 2016, reported \n",
      "electricity usage was conservatively estimated based on maximum contract \n",
      "capacity quantities. We’ve updated our fiscal year 2016 colocation facilities \n",
      "footprint to reflect more accurately Apple’s operational boundaries. Per the WRI \n",
      "Greenhouse Gas Protocol, we’ve removed from our electricity usage and scope \n",
      "2 calculations those emissions associated with colocation facility cooling and \n",
      "building operations.2022 Environmental Progress Report  99 Introduction    Climate Change    Resources    Smarter Chemistry    Engagement    Appendix\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pypdf import PdfReader\n",
    "\n",
    "reader = PdfReader(\"../data/raw/Apple_Environmental_Progress_Report_2022.pdf\")\n",
    "page = reader.pages[98]\n",
    "print(page.extract_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Extra data: line 1 column 4 (char 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mllama_index\u001b[39;00m \u001b[39mimport\u001b[39;00m download_loader\n\u001b[0;32m----> 4\u001b[0m SimpleWebPageReader \u001b[39m=\u001b[39m download_loader(\u001b[39m\"\u001b[39;49m\u001b[39mSimpleWebPageReader\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      6\u001b[0m loader \u001b[39m=\u001b[39m SimpleWebPageReader()\n\u001b[1;32m      7\u001b[0m documents \u001b[39m=\u001b[39m loader\u001b[39m.\u001b[39mload_data(urls\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mhttps://example.com/\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m~/Projects/esg-lab/env/lib/python3.11/site-packages/llama_index/readers/download.py:133\u001b[0m, in \u001b[0;36mdownload_loader\u001b[0;34m(loader_class, loader_hub_url, refresh_cache, use_gpt_index_import, custom_path)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39mif\u001b[39;00m loader_id \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    132\u001b[0m     library_raw_content, _ \u001b[39m=\u001b[39m _get_file_content(loader_hub_url, \u001b[39m\"\u001b[39m\u001b[39m/library.json\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 133\u001b[0m     library \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39;49mloads(library_raw_content)\n\u001b[1;32m    134\u001b[0m     \u001b[39mif\u001b[39;00m loader_class \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m library:\n\u001b[1;32m    135\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mLoader class name not found in library\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39mdecode(detect_encoding(s), \u001b[39m'\u001b[39m\u001b[39msurrogatepass\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[1;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/json/decoder.py:340\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    338\u001b[0m end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m \u001b[39mif\u001b[39;00m end \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(s):\n\u001b[0;32m--> 340\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExtra data\u001b[39m\u001b[39m\"\u001b[39m, s, end)\n\u001b[1;32m    341\u001b[0m \u001b[39mreturn\u001b[39;00m obj\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Extra data: line 1 column 4 (char 3)"
     ]
    }
   ],
   "source": [
    "from llama_index import download_loader\n",
    "\n",
    "\n",
    "SimpleWebPageReader = download_loader(\"SimpleWebPageReader\")\n",
    "\n",
    "loader = SimpleWebPageReader()\n",
    "documents = loader.load_data(urls=['https://example.com/'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Extra data: line 1 column 4 (char 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mllms\u001b[39;00m \u001b[39mimport\u001b[39;00m OpenAI\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchains\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconversation\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmemory\u001b[39;00m \u001b[39mimport\u001b[39;00m ConversationBufferMemory\n\u001b[0;32m----> 6\u001b[0m SimpleWebPageReader \u001b[39m=\u001b[39m download_loader(\u001b[39m\"\u001b[39;49m\u001b[39mSimpleWebPageReader\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      8\u001b[0m loader \u001b[39m=\u001b[39m SimpleWebPageReader()\n\u001b[1;32m      9\u001b[0m documents \u001b[39m=\u001b[39m loader\u001b[39m.\u001b[39mload_data(urls\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mhttps://google.com\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m~/Projects/esg-lab/env/lib/python3.11/site-packages/llama_index/readers/download.py:133\u001b[0m, in \u001b[0;36mdownload_loader\u001b[0;34m(loader_class, loader_hub_url, refresh_cache, use_gpt_index_import, custom_path)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39mif\u001b[39;00m loader_id \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    132\u001b[0m     library_raw_content, _ \u001b[39m=\u001b[39m _get_file_content(loader_hub_url, \u001b[39m\"\u001b[39m\u001b[39m/library.json\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 133\u001b[0m     library \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39;49mloads(library_raw_content)\n\u001b[1;32m    134\u001b[0m     \u001b[39mif\u001b[39;00m loader_class \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m library:\n\u001b[1;32m    135\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mLoader class name not found in library\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39mdecode(detect_encoding(s), \u001b[39m'\u001b[39m\u001b[39msurrogatepass\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[1;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/json/decoder.py:340\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    338\u001b[0m end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m \u001b[39mif\u001b[39;00m end \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(s):\n\u001b[0;32m--> 340\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExtra data\u001b[39m\u001b[39m\"\u001b[39m, s, end)\n\u001b[1;32m    341\u001b[0m \u001b[39mreturn\u001b[39;00m obj\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Extra data: line 1 column 4 (char 3)"
     ]
    }
   ],
   "source": [
    "from llama_index import GPTVectorStoreIndex, download_loader\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "\n",
    "SimpleWebPageReader = download_loader(\"SimpleWebPageReader\")\n",
    "\n",
    "loader = SimpleWebPageReader()\n",
    "documents = loader.load_data(urls=['https://google.com'])\n",
    "index = GPTVectorStoreIndex.from_documents(documents)\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Website Index\",\n",
    "        func=lambda q: index.query(q),\n",
    "        description=f\"Useful when you want answer questions about the text on websites.\",\n",
    "    ),\n",
    "]\n",
    "llm = OpenAI(temperature=0)\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "agent_chain = initialize_agent(\n",
    "    tools, llm, agent=\"zero-shot-react-description\", memory=memory\n",
    ")\n",
    "\n",
    "output = agent_chain.run(input=\"What language is on this website?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'library_raw_content' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m      4\u001b[0m     \u001b[39m# This is where you'd do your JSON loading\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     library \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mloads(library_raw_content)\n\u001b[1;32m      6\u001b[0m \u001b[39mexcept\u001b[39;00m json\u001b[39m.\u001b[39mJSONDecodeError:\n\u001b[1;32m      7\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCaught JSONDecodeError!\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'library_raw_content' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "try:\n",
    "    # This is where you'd do your JSON loading\n",
    "    library = json.loads(library_raw_content)\n",
    "except json.JSONDecodeError:\n",
    "    print(\"Caught JSONDecodeError!\")\n",
    "    print(library_raw_content)  # This will print the content that's causing the issue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
