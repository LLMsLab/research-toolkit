{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accelerating Data Science with Large Language Models (LLMs)\n",
    "\n",
    "**Prompt-Based Development: A New Era**\n",
    "\n",
    "- Prompt-based development allows us to build models much faster, cutting down projects from months to just days.\n",
    "\n",
    "- Instead of lengthy planning stages, we can quickly try out multiple projects in parallel and see what works best, providing more room for creativity and innovation.\n",
    "\n",
    "- A quick proof-of-concept can be built for various NLP tasks like sentiment analysis, email routing, summarizing emails, etc. This allows us to validate both technical and business feasibility.\n",
    "\n",
    "**Implications for Testing and Validation**\n",
    "\n",
    "- With the rise of prompt-based development, traditional test sets may become less crucial for some applications, speeding up the development-deployment cycle.\n",
    "\n",
    "- Deploy models quickly in 'shadow mode', monitor its performance on live data, and let it make real decisions only if performance is satisfactory.\n",
    "\n",
    "References:\n",
    "\n",
    "- https://www.deeplearning.ai/the-batch/how-prompting-is-changing-machine-learning-development/?ref=dl-staging-website.ghost.io\n",
    "- https://www.deeplearning.ai/the-batch/issue-204/\n",
    "\n",
    "## Text-based Problems\n",
    "\n",
    "\n",
    "| Specialized NLP Task                    | Deep Learning Approach                           | Equivalent with Large Language Model | Approach: Zero, One, Few-Shot Learning or Fine-Tuning |\n",
    "|-----------------------------------------|--------------------------------------------------|--------------------------------------|------------------------------------------------------|\n",
    "| Named Entity Recognition (NER)          | LSTM-CRFs or BiLSTM-CRFs                         | Direct inference with GPT             | Few-Shot Learning or Fine-Tuning                      |\n",
    "| Sentiment Analysis                      | Convolutional Neural Networks (CNNs) or LSTMs    | Direct inference with GPT             | Few-Shot Learning or Fine-Tuning                      |\n",
    "| Machine Translation                     | Seq2Seq models with Attention                    | Direct inference with GPT             | Few-Shot Learning or Fine-Tuning                      |\n",
    "| Part of Speech Tagging (POS)            | BiLSTM-CRFs                                      | Direct inference with GPT             | Few-Shot Learning or Fine-Tuning                      |\n",
    "| Text Summarization                      | Seq2Seq models with Attention or Transformers    | Direct inference with GPT             | Few-Shot Learning or Fine-Tuning                      |\n",
    "| Text Generation                         | Recurrent Neural Networks (RNNs) or Transformers | Direct inference with GPT             | Zero-Shot Learning                                    |\n",
    "| Question Answering                      | BiDAF or Transformer-based models                | Direct inference with GPT             | Few-Shot Learning or Fine-Tuning                      |\n",
    "| Dependency Parsing                      | Graph-based or Transition-based models           | Direct inference with GPT             | Few-Shot Learning or Fine-Tuning                      |\n",
    "| Coreference Resolution                  | Deep reinforcement learning-based models         | Direct inference with GPT             | Few-Shot Learning or Fine-Tuning                      |\n",
    "| Text Classification                     | Convolutional Neural Networks (CNNs) or LSTMs    | Direct inference with GPT             | Few-Shot Learning or Fine-Tuning                      |\n",
    "| Semantic Role Labeling (SRL)            | BiLSTM with a CRF layer                          | Direct inference with GPT             | Few-Shot Learning or Fine-Tuning                      |\n",
    "| Relation Extraction                     | Transformer-based models                         | Direct inference with GPT             | Few-Shot Learning or Fine-Tuning                      |\n",
    "\n",
    "Please note that while large language models such as GPT-4 can handle a wide range of tasks via few-shot learning or fine-tuning, the effectiveness of these methods can vary and may not always reach the performance of models specifically designed and trained for the task. For example, for some highly specialized tasks, specific architectures or training methods could be more effective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabular-based Problems\n",
    "\n",
    "| Traditional ML Task                  | Traditional ML Approach              | Equivalent with Large Language Model | Approach: Zero, One, Few-Shot Learning or Fine-Tuning |\n",
    "|--------------------------------------|--------------------------------------|--------------------------------------|------------------------------------------------------|\n",
    "| Classification                       | Decision Trees, SVM, Random Forests  | Direct prediction with GPT           | Few-Shot Learning or Fine-Tuning                      |\n",
    "| Regression                           | Linear Regression, Decision Trees    | Direct prediction with GPT           | Few-Shot Learning or Fine-Tuning                      |\n",
    "| Clustering                           | K-means, Hierarchical clustering     | Not directly applicable              | N/A                                                  |\n",
    "| Dimensionality Reduction             | PCA, t-SNE                           | Not directly applicable              | N/A                                                  |\n",
    "| Anomaly Detection                    | Isolation Forest, One-Class SVM      | Direct prediction with GPT           | Few-Shot Learning or Fine-Tuning                      |\n",
    "| Association Rule Learning            | Apriori, Eclat                       | Not directly applicable              | N/A                                                  |\n",
    "| Reinforcement Learning               | Q-Learning, Deep Q-Network           | Not directly applicable              | N/A                                                  |\n",
    "| Time Series Forecasting              | ARIMA, LSTM                          | Direct prediction with GPT           | Few-Shot Learning or Fine-Tuning                      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text-based Problems\n",
    "\n",
    "| Model Type        | Precision | Recall  | F1 Score  |\n",
    "|-------------------|-----------|---------|-----------|\n",
    "| GPT-4 (Zero-Shot) | 0.92      | 0.97    | 0.95      |\n",
    "| GPT-4 (One-Shot)  | 0.97      | 0.97    | 0.97      |\n",
    "| GPT-4 (Three-Shot)| 0.99      | 0.96    | 0.97      |\n",
    "| LSTM              | 0.97      | 0.96    | 0.97      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
