{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from chromadb.utils import embedding_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.Client()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a collection\n",
    "Collections are where you'll store your embeddings, documents, and any\n",
    "additional metadata. You can create a collection with a \n",
    "name.\n",
    "\n",
    "### Changing the distance function[​](https://docs.trychroma.com/usage-guide#changing-the-distance-function \"Direct link to Changing the distance function\")\n",
    "\n",
    "`create_collection`  also takes an optional  `metadata`  argument which\n",
    "can be used to customize the distance method of the embedding space \n",
    "by setting the value of  `hnsw:space`.\n",
    "\n",
    "Valid options for `hnsw:space` are \"l2\", \"ip\" or \"cosine\". The\n",
    "**default** is \"l2\". The equations for each can be found in the docs for \n",
    "Hnswlib\n",
    "[here](https://github.com/nmslib/hnswlib/tree/master#python-bindings).\n",
    "\n",
    "\n",
    "### Embeddings\n",
    "\n",
    "Chroma provides lightweight wrappers around popular embedding providers,\n",
    "making it easy to use them in your apps. You can set an embedding\n",
    "function when you create a Chroma collection, which will be used\n",
    "automatically, or you can call them directly yourself.\n",
    "\n",
    "\n",
    "To get Chroma's embedding functions, import the\n",
    "chromadb.utils.embedding_functions module.\n",
    "\n",
    "\n",
    "By default, Chroma uses the  [Sentence\n",
    "Transformers](https://www.sbert.net/)  `all-MiniLM-L6-v2`  model to\n",
    "create embeddings. This embedding model can create sentence and document\n",
    "embeddings that can be used for a wide variety of tasks. This embedding\n",
    "function runs locally on your machine, and may require you download the\n",
    "model files (this will happen automatically).\n",
    "\n",
    "\n",
    "```\n",
    "default_ef = embedding_functions.DefaultEmbeddingFunction()\n",
    "```\n",
    "\n",
    "#### Sentence Transformers[​](https://docs.trychroma.com/embeddings#sentence-transformers \"Direct link to Sentence Transformers\")\n",
    "\n",
    "Chroma can also use any  [Sentence Transformers](https://www.sbert.net/)\n",
    "model to create embeddings.\n",
    "\n",
    "\n",
    "```\n",
    "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L6-v2\")\n",
    "```\n",
    "\n",
    "You can pass in an optional  `model_name`  argument, which lets you\n",
    "choose which Sentence Transformers model to use. By default, Chroma uses  \n",
    "`all-MiniLM-L6-v2`. You can see a list of all available models\n",
    "[here](https://www.sbert.net/docs/pretrained_models.html).\n",
    "\n",
    "Let's use for example the `all-mpnet-base-v2` from sentence transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)a8e1d/.gitattributes: 100%|██████████| 1.18k/1.18k [00:00<00:00, 4.40MB/s]\n",
      "Downloading (…)_Pooling/config.json: 100%|██████████| 190/190 [00:00<00:00, 2.14MB/s]\n",
      "Downloading (…)b20bca8e1d/README.md: 100%|██████████| 10.6k/10.6k [00:00<00:00, 35.6MB/s]\n",
      "Downloading (…)0bca8e1d/config.json: 100%|██████████| 571/571 [00:00<00:00, 7.13MB/s]\n",
      "Downloading (…)ce_transformers.json: 100%|██████████| 116/116 [00:00<00:00, 634kB/s]\n",
      "Downloading (…)e1d/data_config.json: 100%|██████████| 39.3k/39.3k [00:00<00:00, 49.8MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 438M/438M [00:10<00:00, 43.6MB/s] \n",
      "Downloading (…)nce_bert_config.json: 100%|██████████| 53.0/53.0 [00:00<00:00, 715kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 239/239 [00:00<00:00, 1.30MB/s]\n",
      "Downloading (…)a8e1d/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 6.93MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 363/363 [00:00<00:00, 2.51MB/s]\n",
      "Downloading (…)8e1d/train_script.py: 100%|██████████| 13.1k/13.1k [00:00<00:00, 66.6MB/s]\n",
      "Downloading (…)b20bca8e1d/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 24.1MB/s]\n",
      "Downloading (…)bca8e1d/modules.json: 100%|██████████| 349/349 [00:00<00:00, 1.97MB/s]\n"
     ]
    }
   ],
   "source": [
    "sentence_transformer_ef = (\n",
    "    embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "        model_name=\"all-mpnet-base-v2\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emb_fn = \"all-mpnet-base-v2\" # embedding model. Default all-MiniLM-L6-v2\n",
    "dis_fn = \"cosine\" # distance function. Default l2 \n",
    "\n",
    "collection = chroma_client.create_collection(\n",
    "    name=\"cosmic_chronicles\",\n",
    "    embedding_function=sentence_transformer_ef,\n",
    "    metadata={\"hnsw:space\": dis_fn},\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete a collection\n",
    "\n",
    "Delete a collection and all associated embeddings, documents, and\n",
    "metadata. ⚠️ This is destructive and not reversible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client.delete_collection(name=\"cosmic_chronicles\") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add some text documents to the collection\n",
    "Chroma will store your text, and handle tokenization, embedding, and\n",
    "indexing automatically.\n",
    "\n",
    " Let's create a collection named \"Cosmic Chronicles\". This collection\n",
    " will contain short summaries of interesting astronomical \n",
    " events and discoveries. Here's how you can add these documents to your\n",
    " collection:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.add(\n",
    "    documents=[\n",
    "        \"Discovery of water on Mars: In 2020, scientists confirmed the existence of underground lakes on Mars.\",\n",
    "        \"First image of a black hole: In 2019, the Event Horizon Telescope captured the first image of a black hole located in the M87 galaxy.\",\n",
    "        \"Voyager 1 enters interstellar space: In 2012, the Voyager 1 spacecraft became the first human-made object to enter interstellar space.\",\n",
    "        \"Detection of gravitational waves: In 2015, the LIGO observatory made the first direct observation of gravitational waves, confirming a major prediction of Albert Einstein's general theory of relativity.\",\n",
    "        \"The Hubble Space Telescope: Launched in 1990, the Hubble Space Telescope has provided some of the most detailed images of distant galaxies, nebulae, and stars.\",\n",
    "        \"The Kepler Mission: Launched in 2009, the Kepler space telescope has discovered more than 2,600 confirmed planets outside our solar system.\",\n",
    "        \"The discovery of the first exoplanet: In 1995, the first exoplanet orbiting a sun-like star, 51 Pegasi b, was discovered.\",\n",
    "    ],\n",
    "    metadatas=[\n",
    "        {\"source\": \"NASA\"},\n",
    "        {\"source\": \"Event Horizon Telescope\"},\n",
    "        {\"source\": \"NASA\"},\n",
    "        {\"source\": \"LIGO\"},\n",
    "        {\"source\": \"NASA\"},\n",
    "        {\"source\": \"NASA\"},\n",
    "        {\"source\": \"Observatoire de Genève, Michel Mayor, Didier Queloz\"},\n",
    "    ],\n",
    "    ids=[\"doc1\", \"doc2\", \"doc3\", \"doc4\", \"doc5\", \"doc6\", \"doc7\"],\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query the collection\n",
    "You can query the collection with a list of query texts, and Chroma will\n",
    "return the n most similar results. It's that easy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['doc7', 'doc6', 'doc4', 'doc5', 'doc1', 'doc3', 'doc2']],\n",
       " 'embeddings': None,\n",
       " 'documents': [['The discovery of the first exoplanet: In 1995, the first exoplanet orbiting a sun-like star, 51 Pegasi b, was discovered.',\n",
       "   'The Kepler Mission: Launched in 2009, the Kepler space telescope has discovered more than 2,600 confirmed planets outside our solar system.',\n",
       "   \"Detection of gravitational waves: In 2015, the LIGO observatory made the first direct observation of gravitational waves, confirming a major prediction of Albert Einstein's general theory of relativity.\",\n",
       "   'The Hubble Space Telescope: Launched in 1990, the Hubble Space Telescope has provided some of the most detailed images of distant galaxies, nebulae, and stars.',\n",
       "   'Discovery of water on Mars: In 2020, scientists confirmed the existence of underground lakes on Mars.',\n",
       "   'Voyager 1 enters interstellar space: In 2012, the Voyager 1 spacecraft became the first human-made object to enter interstellar space.',\n",
       "   'First image of a black hole: In 2019, the Event Horizon Telescope captured the first image of a black hole located in the M87 galaxy.']],\n",
       " 'metadatas': [[{'source': 'Observatoire de Genève, Michel Mayor, Didier Queloz'},\n",
       "   {'source': 'NASA'},\n",
       "   {'source': 'LIGO'},\n",
       "   {'source': 'NASA'},\n",
       "   {'source': 'NASA'},\n",
       "   {'source': 'NASA'},\n",
       "   {'source': 'Event Horizon Telescope'}]],\n",
       " 'distances': [[0.25299882888793945,\n",
       "   0.3446120023727417,\n",
       "   0.6365892887115479,\n",
       "   0.6816555261611938,\n",
       "   0.6843814849853516,\n",
       "   0.7076455354690552,\n",
       "   0.7849907279014587]]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = collection.query(\n",
    "    query_texts=[\"Discovery of exoplanets\"], n_results=7\n",
    ")\n",
    "results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default data stored in Chroma is ephemeral making it easy to\n",
    "prototype scripts. It's easy to make Chroma persistent so you can reuse\n",
    "every collection you create and add more documents to it later. It will\n",
    "load your data automatically when you start the client, and save it\n",
    "automatically when you close it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Collection.peek of Collection(name=cosmic_chronicles)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.peek"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings in the context of LLMs\n",
    "\n",
    "The choice between BERT-based embeddings and GPT-based embeddings for data preparation in Large Language Models (LLMs) applications depends on the specific requirements of the task at hand. Here are some general guidelines:\n",
    "\n",
    "**BERT-based embeddings** are typically used when the task requires understanding the context from both directions (left and right) around a word. BERT is a bidirectional model, meaning it looks at the context from both directions. This makes BERT-based embeddings very effective for tasks like:\n",
    "\n",
    "- **Question Answering**: If you're building a system to answer questions based on a given text, BERT-based embeddings can help the model understand the context of the question and find the most relevant answer in the text.\n",
    "- **Named Entity Recognition**: If you're trying to identify and categorize entities in a text, BERT-based embeddings can help the model understand the context around each entity.\n",
    "- **Sentiment Analysis**: If you're analyzing the sentiment expressed in a piece of text, BERT-based embeddings can help the model understand the nuances of the sentiment.\n",
    "\n",
    "**GPT-based embeddings** are typically used when the task involves generating text. GPT is a unidirectional model, meaning it generates a sentence from left to right. This makes GPT-based embeddings very effective for tasks like:\n",
    "\n",
    "- **Text Generation**: If you're building a system to generate human-like text, GPT-based embeddings can help the model generate contextually relevant sentences.\n",
    "- **Machine Translation**: If you're translating text from one language to another, GPT-based embeddings can help the model generate fluent translations.\n",
    "- **Summarization**: If you're summarizing a longer piece of text, GPT-based embeddings can help the model generate a concise and relevant summary.\n",
    "\n",
    "In general, the choice between BERT-based and GPT-based embeddings depends on whether the task requires understanding the context around a word (in which case BERT-based embeddings are typically used) or generating text (in which case GPT-based embeddings are typically used). However, both types of embeddings are very versatile and can be used for a wide range of tasks.\n",
    "\n",
    "### Sentence Transformers\n",
    "\n",
    "SentenceTransformers is a Python library that provides an easy-to-use interface for generating sentence embeddings using transformer-based models, which includes BERT and its variants.\n",
    "\n",
    "The library is built on top of the Hugging Face's Transformers library, which provides the underlying transformer models like BERT, RoBERTa, DistilBERT, etc. SentenceTransformers adds a pooling operation on top of these models to generate a single vector for the entire input sequence (sentence).\n",
    "\n",
    "The key innovation of SentenceTransformers is that it modifies the pretraining objective of these transformer models to optimize them for generating sentence embeddings. This is done using a siamese or triplet network structure, where the aim is to bring closer the embeddings of semantically similar sentences and separate the embeddings of semantically dissimilar sentences.\n",
    "\n",
    "So, in summary, SentenceTransformers provides a family of models for generating sentence embeddings, and these models are based on BERT and other transformer architectures. The embeddings generated by these models are bidirectional, meaning they capture the context from both directions (left and right) around each word in a sentence."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of other popular embeddings\n",
    "\n",
    "\n",
    "Embedding techniques are used to convert text data into numerical vectors that can be processed by machine learning algorithms. Here are some of the main embedding techniques:\n",
    "\n",
    "1. **Word2Vec**: Developed by Google, Word2Vec is a two-layer neural network that processes text by vectorizing words. Its input is a text corpus and its output is a set of vectors, with each vector representing a word in the corpus. Word2Vec uses either the Continuous Bag of Words (CBOW) or Skip-Gram method to predict words within a certain context.\n",
    "\n",
    "2. **GloVe (Global Vectors for Word Representation)**: Developed by Stanford, GloVe is an unsupervised learning algorithm that obtains vector representations for words. It does this by aggregating global word-word co-occurrence statistics from a corpus and then mapping these statistics into a word vector space.\n",
    "\n",
    "3. **FastText**: Developed by Facebook's AI Research lab, FastText is an extension to Word2Vec. Instead of feeding individual words into the neural network, FastText treats each word as a bag of character n-grams. This allows it to capture the meaning of shorter words and allows it to understand suffixes and prefixes.\n",
    "\n",
    "4. **BERT (Bidirectional Encoder Representations from Transformers)**: Developed by Google, BERT is a transformer-based machine learning technique for natural language processing. Unlike Word2Vec and GloVe, which generate a single static embedding for each word in the vocabulary, BERT generates context-dependent embeddings. This means that the word embeddings are influenced by the other words in the sentence.\n",
    "\n",
    "5. **GPT (Generative Pretrained Transformer)**: Developed by OpenAI, GPT is another transformer-based model. While BERT is designed to handle tasks that require understanding both the left and right context of a word (bidirectional understanding), GPT is designed to handle tasks that require understanding only the right context (unidirectional understanding). GPT-3, the latest version of GPT as of my knowledge cutoff in September 2021, is particularly powerful and can generate human-like text given a certain prompt.\n",
    "\n",
    "6. **ELMo (Embeddings from Language Models)**: Developed by Allen AI, ELMo is a deep contextualized word representation that models both complex characteristics of word use (e.g., syntax and semantics), and how these uses vary across linguistic contexts (i.e., polysemy). Unlike traditional word embeddings such as Word2Vec and GloVe, which generate a single static embedding for each word, ELMo generates embeddings dynamically based on the word context.\n",
    "\n",
    "These are just a few examples of the many techniques available for generating word embeddings. The choice of technique depends on the specific requirements of the task at hand."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select a distance method\n",
    "\n",
    "In the context of embeddings and Large Language Models (LLMs) applications, the choice of distance metric depends on the specific requirements of the task and the nature of the embeddings. Here's a brief overview of the three distance metrics you mentioned:\n",
    "\n",
    "1. **Euclidean Distance (L2)**: This is the straight-line distance between two points in a space. It's often used when the magnitude of the embeddings is important. However, in high-dimensional spaces (like the ones typically produced by LLMs), Euclidean distance can suffer from the \"curse of dimensionality,\" where all points appear to be roughly equidistant to one another.\n",
    "\n",
    "2. **Cosine Similarity**: This measures the cosine of the angle between two vectors. It's often used when the orientation (or direction) of the embeddings is more important than their magnitude. Cosine similarity is particularly useful for text data, as it can capture the semantic similarity between documents (or words) regardless of their size.\n",
    "\n",
    "3. **Inner Product (IP)**: This is the sum of the product of the corresponding entries of the two sequences of numbers. In the context of embeddings, it can be used to measure the similarity between two vectors, but unlike cosine similarity, it also takes into account the magnitude of the vectors.\n",
    "\n",
    "In general, if you're working with text data and using embeddings to capture semantic similarity, cosine similarity is often a good choice. However, if the magnitude of the embeddings is also important (for example, if you're using embeddings to capture the importance or frequency of certain features), then you might want to consider using the inner product or Euclidean distance.\n",
    "\n",
    "It's also worth noting that the choice of distance metric can also depend on the specific embedding technique you're using. Some techniques might produce embeddings where certain distance metrics are more appropriate than others. For example, Word2Vec embeddings are often compared using cosine similarity, while BERT embeddings might be compared using the inner product. \n",
    "\n",
    "As always, the best way to determine which distance metric to use is to\n",
    "experiment with different options and see which one works best for your\n",
    "specific task.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of embedding and distances\n",
    "\n",
    "Here's a simplified table that outlines some common applications of Large Language Models (LLMs), along with a suggested embedding technique and distance method for each. Please note that these are general suggestions and the best choice can vary depending on the specifics of the task, the available data, and the computational resources.\n",
    "\n",
    "| Application | Embedding Technique | Distance Method |\n",
    "|-------------|---------------------|-----------------|\n",
    "| Text Classification | BERT | Cosine |\n",
    "| Sentiment Analysis | BERT | Cosine |\n",
    "| Named Entity Recognition | BERT | Cosine |\n",
    "| Question Answering | BERT | Cosine |\n",
    "| Text Generation | GPT | Inner Product |\n",
    "| Machine Translation | GPT | Inner Product |\n",
    "| Summarization | GPT | Inner Product |\n",
    "| Semantic Search | BERT or Sentence-BERT | Cosine |\n",
    "| Chatbots and Conversational Agents | GPT | Inner Product |\n",
    "\n",
    "In general, BERT and its variants (like Sentence-BERT for sentence embeddings) are often used for tasks that require understanding the context of a sentence, with cosine similarity as the distance method to capture semantic similarity. On the other hand, GPT and its variants are often used for tasks that involve generating text, with the inner product as the distance method to capture the similarity in the generated text."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"content\": \"Hello! How can I assist you today?\\n\",\n",
      "  \"role\": \"assistant\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "completion = openai.ChatCompletion.create(\n",
    "  model=\"gpt-4\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello!\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"content\": \"(A) Spam\",\n",
      "  \"role\": \"assistant\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "completion = openai.ChatCompletion.create(\n",
    "  model=\"gpt-4\",\n",
    "  messages=[\n",
    "    {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Question: Given the following email content, is the email spam or not spam? Email Content: Hi, just checking in about our meeting tomorrow. Answer Choices: (A) Spam, (B) Not Spam.\"\n",
    "    },\n",
    "    {\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": \"Answer: (B) Not Spam\"\n",
    "    },\n",
    "    {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Question: Given the following email content, is the email spam or not spam? Email Content: Congratulations! You've been selected for a free vacation! Click here now! Answer Choices: (A) Spam, (B) Not Spam.\"\n",
    "    },\n",
    "    {\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": \"Answer: (A) Spam\"\n",
    "    },\n",
    "    {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Question: Given the following email content, is the email spam or not spam? Email Content: Dear user, you have won a million dollars! Click here to claim your prize. Answer Choices: (A) Spam, (B) Not Spam.\"\n",
    "    },\n",
    "    {\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": \"Answer: \"\n",
    "    }\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "import openai\n",
    "\n",
    "categories = ['rec.sport.baseball', 'rec.sport.hockey']\n",
    "sports_dataset = fetch_20newsgroups(subset='train', shuffle=True, random_state=42, categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: dougb@comm.mot.com (Doug Bank)\n",
      "Subject: Re: Info needed for Cleveland tickets\n",
      "Reply-To: dougb@ecs.comm.mot.com\n",
      "Organization: Motorola Land Mobile Products Sector\n",
      "Distribution: usa\n",
      "Nntp-Posting-Host: 145.1.146.35\n",
      "Lines: 17\n",
      "\n",
      "In article <1993Apr1.234031.4950@leland.Stanford.EDU>, bohnert@leland.Stanford.EDU (matthew bohnert) writes:\n",
      "\n",
      "|> I'm going to be in Cleveland Thursday, April 15 to Sunday, April 18.\n",
      "|> Does anybody know if the Tribe will be in town on those dates, and\n",
      "|> if so, who're they playing and if tickets are available?\n",
      "\n",
      "The tribe will be in town from April 16 to the 19th.\n",
      "There are ALWAYS tickets available! (Though they are playing Toronto,\n",
      "and many Toronto fans make the trip to Cleveland as it is easier to\n",
      "get tickets in Cleveland than in Toronto.  Either way, I seriously\n",
      "doubt they will sell out until the end of the season.)\n",
      "\n",
      "-- \n",
      "Doug Bank                       Private Systems Division\n",
      "dougb@ecs.comm.mot.com          Motorola Communications Sector\n",
      "dougb@nwu.edu                   Schaumburg, Illinois\n",
      "dougb@casbah.acns.nwu.edu       708-576-8207                    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sports_dataset['data'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rec.sport.baseball'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sports_dataset.target_names[sports_dataset['target'][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total examples: 1197, Baseball examples: 597, Hockey examples: 600\n"
     ]
    }
   ],
   "source": [
    "len_all, len_baseball, len_hockey = len(sports_dataset.data), len([e for e in sports_dataset.target if e == 0]), len([e for e in sports_dataset.target if e == 1])\n",
    "print(f\"Total examples: {len_all}, Baseball examples: {len_baseball}, Hockey examples: {len_hockey}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: dougb@comm.mot.com (Doug Bank)\\nSubject:...</td>\n",
       "      <td>baseball</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: gld@cunixb.cc.columbia.edu (Gary L Dare)...</td>\n",
       "      <td>hockey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: rudy@netcom.com (Rudy Wade)\\nSubject: Re...</td>\n",
       "      <td>baseball</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: monack@helium.gas.uug.arizona.edu (david...</td>\n",
       "      <td>hockey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Subject: Let it be Known\\nFrom: &lt;ISSBTL@BYUVM....</td>\n",
       "      <td>baseball</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt completion\n",
       "0  From: dougb@comm.mot.com (Doug Bank)\\nSubject:...   baseball\n",
       "1  From: gld@cunixb.cc.columbia.edu (Gary L Dare)...     hockey\n",
       "2  From: rudy@netcom.com (Rudy Wade)\\nSubject: Re...   baseball\n",
       "3  From: monack@helium.gas.uug.arizona.edu (david...     hockey\n",
       "4  Subject: Let it be Known\\nFrom: <ISSBTL@BYUVM....   baseball"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "labels = [sports_dataset.target_names[x].split('.')[-1] for x in sports_dataset['target']]\n",
    "texts = [text.strip() for text in sports_dataset['data']]\n",
    "df = pd.DataFrame(zip(texts, labels), columns = ['prompt','completion']) #[:300]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "completion = openai.ChatCompletion.create(\n",
    "  model=\"gpt-4\",\n",
    "  messages=[\n",
    "    {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Question: Given the following email content, is the email spam or not spam? Email Content: Hi, just checking in about our meeting tomorrow. Answer Choices: (A) Spam, (B) Not Spam.\"\n",
    "    },\n",
    "    {\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": \"Answer: (B) Not Spam\"\n",
    "    },\n",
    "    {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Question: Given the following email content, is the email spam or not spam? Email Content: Congratulations! You've been selected for a free vacation! Click here now! Answer Choices: (A) Spam, (B) Not Spam.\"\n",
    "    },\n",
    "    {\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": \"Answer: (A) Spam\"\n",
    "    },\n",
    "    {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Question: Given the following email content, is the email spam or not spam? Email Content: Dear user, you have won a million dollars! Click here to claim your prize. Answer Choices: (A) Spam, (B) Not Spam.\"\n",
    "    },\n",
    "    {\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": \"Answer: \"\n",
    "    }\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
